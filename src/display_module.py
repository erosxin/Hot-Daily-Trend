# src/display_module.py
import logging
from typing import List
from src.data_models import Article
import os
from pathlib import Path

from datetime import datetime, timezone

logger = logging.getLogger(__name__)

class DisplayModule:
    def __init__(self):
        """åˆå§‹åŒ–å±•ç¤ºæ¨¡å—"""
        pass

    def generate_mindmap_markdown(self, articles: List[Article]) -> str:
        """
        å°†æ–‡ç« åˆ—è¡¨è½¬æ¢ä¸º Mind Map (Markmap) å…¼å®¹çš„ Markdown æ ¼å¼ã€‚
        
        ç»“æ„ï¼š
        # ä¸»é¢˜ (æ‰€æœ‰æ–‡ç« )
        ## æ¥æº1
        ### æ–‡ç« 1 (æ ‡é¢˜)
        - ç®€è¿°
        - å®ä½“
        - æ ‡ç­¾
        ### æ–‡ç« 2
        ## æ¥æº2
        ...
        
        :param articles: å¾…å±•ç¤ºçš„æ–‡ç« åˆ—è¡¨
        :return: Markdown æ ¼å¼çš„å­—ç¬¦ä¸²
        """
        if not articles:
            return "# No Articles Found"

        mindmap_output = ["# AI News Feed Overview"]
        
        # æŒ‰æ¥æºåˆ†ç»„æ–‡ç« 
        articles_by_source = {}
        for article in articles:
            if article.source not in articles_by_source:
                articles_by_source[article.source] = []
            articles_by_source[article.source].append(article)
        
        # æŒ‰æ¥æºåç§°æ’åº
        for source in sorted(articles_by_source.keys()):
            source_articles = articles_by_source[source]
            mindmap_output.append(f"## {source}")
            
            # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            def get_timestamp(article):
                """è·å–æ–‡ç« å‘å¸ƒæ—¶é—´çš„æ—¶é—´æˆ³ï¼Œç”¨äºæ’åº"""
                if not article.published:
                    return datetime.min.timestamp()
                
                try:
                    if isinstance(article.published, datetime):
                        # å¦‚æœæ˜¯ datetime å¯¹è±¡ï¼Œè½¬æ¢ä¸º UTC æ—¶é—´æˆ³
                        dt = article.published
                        if dt.tzinfo is None:
                            dt = dt.replace(tzinfo=timezone.utc)
                        else:
                            dt = dt.astimezone(timezone.utc)
                        return dt.timestamp()
                    elif isinstance(article.published, str):
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè§£æä¸º datetime å†è½¬æ¢
                        published_str = article.published.replace('Z', '+00:00')
                        dt = datetime.fromisoformat(published_str).astimezone(timezone.utc)
                        return dt.timestamp()
                    else:
                        return datetime.min.timestamp()
                except Exception as e:
                    logger.warning(f"Error parsing published date for article '{article.title}': {e}")
                    return datetime.min.timestamp()
            
            sorted_source_articles = sorted(
                source_articles,
                key=get_timestamp,
                reverse=True
            )
            
            for article in sorted_source_articles:
                # æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å… Markdown è§£æé—®é¢˜
                clean_title = article.title.replace('[', '').replace(']', '').replace('(', '').replace(')', '')
                mindmap_output.append(f"### {clean_title}")
                
                # æ·»åŠ æ‘˜è¦/ç®€è¿°
                summary = article.summary[:150] + '...' if article.summary and len(article.summary) > 150 else (article.summary or 'No summary')
                mindmap_output.append(f"- **Summary**: {summary}")
                
                # æ·»åŠ å®ä½“ï¼ˆentities ç°åœ¨æ˜¯ Dict[str, List[str]]ï¼‰
                if article.entities:
                    # å±•å¹³å®ä½“å­—å…¸ä¸ºåˆ—è¡¨
                    entity_list = []
                    for entity_type, entity_values in article.entities.items():
                        entity_list.extend(entity_values[:2])  # æ¯ç§ç±»å‹æœ€å¤š2ä¸ª
                        if len(entity_list) >= 5:
                            break
                    if entity_list:
                        mindmap_output.append(f"- **Entities**: {', '.join(entity_list[:5])}")  # é™åˆ¶å®ä½“æ•°é‡
                
                # æ·»åŠ æ ‡ç­¾
                if article.main_tags:
                    mindmap_output.append(f"- **Tags**: {', '.join(article.main_tags)}")
                
                # æ·»åŠ é“¾æ¥ï¼ˆå°† HttpUrl è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
                link_str = str(article.link)
                # å¦‚æœé“¾æ¥å¾ˆé•¿ï¼Œæ˜¾ç¤ºç¼©çŸ­ç‰ˆæœ¬
                if len(link_str) > 60:
                    display_link = f"{link_str[:30]}...{link_str[-20:]}"
                else:
                    display_link = link_str
                mindmap_output.append(f"- **Link**: [{display_link}]({link_str})")
        
        logger.info(f"Generated Mind Map Markdown output for {len(articles)} articles.")
        return "\n".join(mindmap_output)

    def generate_timeline_markdown(self, articles: List[Article]) -> str:
        """
        å°†æ–‡ç« åˆ—è¡¨è½¬æ¢ä¸ºæ—¶é—´è½´å…¼å®¹çš„ Markdown æ ¼å¼ã€‚
        
        ç»“æ„ï¼š
        # äº‹ä»¶æ—¶é—´è½´
        ## å¹´ä»½-æœˆä»½
        ### æ—¥æœŸ
        - [æ—¶é—´] æ¥æº: æ ‡é¢˜ (ç®€è¿°)
        - å®ä½“ï¼Œæ ‡ç­¾
        
        :param articles: å¾…å±•ç¤ºçš„æ–‡ç« åˆ—è¡¨
        :return: Markdown æ ¼å¼çš„å­—ç¬¦ä¸²
        """
        if not articles:
            return "# No Articles Found"

        timeline_output = ["# AI News Feed Timeline"]
        
        # æŒ‰æ—¶é—´æ’åºæ–‡ç« ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        sorted_articles = []
        for article in articles:
            try:
                # å¤„ç†ä¸åŒçš„æ—¶é—´æ ¼å¼
                if isinstance(article.published, datetime):
                    published_dt = article.published.astimezone(timezone.utc) if article.published.tzinfo else article.published.replace(tzinfo=timezone.utc)
                elif isinstance(article.published, str):
                    published_str = article.published
                    if 'Z' in published_str:
                        published_str = published_str.replace('Z', '+00:00')
                    published_dt = datetime.fromisoformat(published_str).astimezone(timezone.utc)
                else:
                    # å¦‚æœç±»å‹æœªçŸ¥ï¼Œå°è¯•è½¬æ¢
                    published_dt = datetime.now(timezone.utc)
                sorted_articles.append((published_dt, article))
            except Exception as e:
                logger.warning(f"Error parsing date for article '{article.title}': {e}")
                # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                sorted_articles.append((datetime.now(timezone.utc), article))
        
        sorted_articles.sort(key=lambda x: x[0].timestamp(), reverse=True)
        
        current_month = ""
        current_day = ""

        for published_dt, article in sorted_articles:
            year_month = published_dt.strftime("%Yå¹´%mæœˆ")
            day_str = published_dt.strftime("%Yå¹´%mæœˆ%dæ—¥")
            time_str = published_dt.strftime("%H:%M")

            if year_month != current_month:
                timeline_output.append(f"\n## {year_month}")
                current_month = year_month
                current_day = ""  # é‡ç½®å¤©æ•°ï¼Œå› ä¸ºæœˆä»½å˜äº†

            if day_str != current_day:
                timeline_output.append(f"\n### {day_str}")
                current_day = day_str

            # æ¸…ç†æ ‡é¢˜ä¸­çš„ç‰¹æ®Šå­—ç¬¦
            clean_title = article.title.replace('[', '').replace(']', '')
            line_prefix = f"- **[{time_str}]** {article.source}: {clean_title}"
            
            # æ·»åŠ æ‘˜è¦
            summary = article.summary[:100] + '...' if article.summary and len(article.summary) > 100 else (article.summary or 'No summary')
            timeline_output.append(f"{line_prefix}")
            timeline_output.append(f"  - {summary}")
            
            # æ·»åŠ è¯¦ç»†ä¿¡æ¯
            details = []
            if article.entities:
                # å±•å¹³å®ä½“å­—å…¸ä¸ºåˆ—è¡¨
                entity_list = []
                for entity_type, entity_values in article.entities.items():
                    entity_list.extend(entity_values[:2])  # æ¯ç§ç±»å‹æœ€å¤š2ä¸ª
                    if len(entity_list) >= 5:
                        break
                if entity_list:
                    details.append(f"**å®ä½“**: {', '.join(entity_list[:5])}")
            if article.main_tags:
                details.append(f"**æ ‡ç­¾**: {', '.join(article.main_tags)}")
            if details:
                timeline_output.append(f"  - {', '.join(details)}")
            
            # æ·»åŠ é“¾æ¥ï¼ˆå°† HttpUrl è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
            link_str = str(article.link)
            timeline_output.append(f"  - [æŸ¥çœ‹åŸæ–‡]({link_str})")
            timeline_output.append("")  # ç©ºè¡Œåˆ†éš”
        
        logger.info(f"Generated Timeline Markdown output for {len(articles)} articles.")
        return "\n".join(timeline_output)

    def generate_summary_statistics(self, articles: List[Article]) -> str:
        """
        ç”Ÿæˆæ–‡ç« ç»Ÿè®¡æ‘˜è¦ã€‚
        
        :param articles: å¾…ç»Ÿè®¡çš„æ–‡ç« åˆ—è¡¨
        :return: Markdown æ ¼å¼çš„ç»Ÿè®¡æ‘˜è¦
        """
        if not articles:
            return "## Statistics\n- No articles found"
        
        stats_output = ["## Statistics"]
        stats_output.append(f"- **Total Articles**: {len(articles)}")
        
        # æŒ‰æ¥æºç»Ÿè®¡
        source_counts = {}
        for article in articles:
            source_counts[article.source] = source_counts.get(article.source, 0) + 1
        
        stats_output.append(f"\n### By Source")
        for source, count in sorted(source_counts.items(), key=lambda x: x[1], reverse=True):
            stats_output.append(f"- {source}: {count}")
        
        # æŒ‰æ ‡ç­¾ç»Ÿè®¡
        tag_counts = {}
        for article in articles:
            for tag in article.main_tags:
                tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        if tag_counts:
            stats_output.append(f"\n### By Tag")
            for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True):
                stats_output.append(f"- {tag}: {count}")

        # æŒ‰å®ä½“ç»Ÿè®¡ï¼ˆentities ç°åœ¨æ˜¯ Dict[str, List[str]]ï¼‰
        entity_counts = {}
        for article in articles:
            if article.entities:
                for entity_type, entity_values in article.entities.items():
                    for entity in entity_values:
                        entity_counts[entity] = entity_counts.get(entity, 0) + 1

        if entity_counts:
            stats_output.append(f"\n### Top Entities")
            for entity, count in sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
                stats_output.append(f"- {entity}: {count}")

        return "\n".join(stats_output)

    def generate_email_html(self, articles: List[Article], base_url: str) -> str:
        base_url = (base_url or "").rstrip("/")

        # Sort by heat_score then published
        if not articles:
            return "<h2>ä»Šæ—¥æš‚æ— å¯ç”¨AIæ–°é—»</h2>"

        def _published_ts(article: Article) -> float:
            if not article.published:
                return 0.0
            if isinstance(article.published, datetime):
                dt = article.published
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                else:
                    dt = dt.astimezone(timezone.utc)
                return dt.timestamp()
            if isinstance(article.published, str):
                try:
                    published_str = article.published.replace('Z', '+00:00')
                    dt = datetime.fromisoformat(published_str).astimezone(timezone.utc)
                    return dt.timestamp()
                except Exception:
                    return 0.0
            return 0.0

        def score_key(a: Article):
            return (a.heat_score or 0, _published_ts(a))

        top_articles = sorted(articles, key=score_key, reverse=True)

        # Trend radar
        trend_counts = {}
        for a in articles:
            tag = a.trend_tag or (a.main_tags[0] if a.main_tags else "å…¶ä»–")
            trend_counts[tag] = trend_counts.get(tag, 0) + 1

        trend_items = "".join([f"<li>{k}: {v}</li>" for k, v in sorted(trend_counts.items(), key=lambda x: x[1], reverse=True)])

        rows = []
        for a in top_articles:
            heat = f"{int(a.heat_score)}" if a.heat_score is not None else "-"
            summary = a.summary_zh or a.summary or ""
            plain_summary = a.plain_summary or ""
            key_points = "".join([f"<li>{p}</li>" for p in (a.key_points or [])[:3]])
            fav_link = f"{base_url}/favorite.html?id={a.id}" if a.id else "#"
            
            # å¦‚æœæœ‰é€šä¿—æ€»ç»“ï¼Œæ·»åŠ åˆ°æ˜¾ç¤ºä¸­
            plain_summary_html = f"<div style='font-size:13px;color:#2563eb;margin:8px 0;padding:8px;background:#f0f7ff;border-radius:4px;'>ğŸ’¡ {plain_summary}</div>" if plain_summary else ""
            
            rows.append(
                f"""
                <div style='padding:12px 0;border-bottom:1px solid #eee;'>
                  <div style='font-size:16px;font-weight:600;margin-bottom:6px;'>{a.title}</div>
                  <div style='color:#666;font-size:12px;margin-bottom:8px;'>æ¥æº: {a.source} | çƒ­åº¦: {heat}</div>
                  <div style='font-size:14px;margin-bottom:6px;'>{summary}</div>
                  {plain_summary_html}
                  <ul style='margin:4px 0 8px 18px;padding:0;'>{key_points}</ul>
                  <div style='font-size:13px;'>
                    <a href='{str(a.link)}' target='_blank'>æŸ¥çœ‹åŸæ–‡</a> | 
                    <a href='{fav_link}' target='_blank'>æ”¶è—å¹¶ç”Ÿæˆç®€æ</a>
                  </div>
                </div>
                """
            )

        html = f"""
        <html><body style='font-family:Arial,Helvetica,sans-serif;'>
          <h2>æ¯æ—¥AIè¶‹åŠ¿ç®€æŠ¥</h2>
          <h3>è¶‹åŠ¿é›·è¾¾</h3>
          <ul>{trend_items}</ul>
          <h3>ä»Šæ—¥è¦é—»</h3>
          {''.join(rows)}
        </body></html>
        """
        return html

    def generate_static_site(self, output_dir: Path, articles: List[Article], base_url: str, supabase_url: str, supabase_anon_key: str) -> None:
        base_url = (base_url or "").rstrip("/")
        output_dir.mkdir(parents=True, exist_ok=True)

        # Main index page
        items_html = []
        for a in articles:
            heat = f"{int(a.heat_score)}" if a.heat_score is not None else "-"
            summary = a.summary_zh or a.summary or ""
            plain_summary = a.plain_summary or ""
            fav_link = f"{base_url}/favorite.html?id={a.id}" if a.id else "#"
            
            # å¦‚æœæœ‰é€šä¿—æ€»ç»“ï¼Œæ·»åŠ åˆ°æ˜¾ç¤ºä¸­
            plain_summary_html = f"<div class='plain-summary'>ğŸ’¡ {plain_summary}</div>" if plain_summary else ""
            
            items_html.append(
                f"""
                <div class='item'>
                  <div class='title'>{a.title}</div>
                  <div class='meta'>æ¥æº: {a.source} | çƒ­åº¦: {heat}</div>
                  <div class='summary'>{summary}</div>
                  {plain_summary_html}
                  <div class='links'>
                    <a href='{str(a.link)}' target='_blank'>æŸ¥çœ‹åŸæ–‡</a>
                    <a href='{fav_link}' target='_blank'>æ”¶è—å¹¶ç”Ÿæˆç®€æ</a>
                  </div>
                </div>
                """
            )

        index_html = f"""
        <html>
        <head>
          <meta charset='utf-8' />
          <title>AI Daily Trend</title>
          <style>
            body {{ font-family: Arial, sans-serif; margin: 24px; color: #111; }}
            .item {{ padding: 12px 0; border-bottom: 1px solid #eee; }}
            .title {{ font-size: 18px; font-weight: 600; margin-bottom: 6px; }}
            .meta {{ font-size: 12px; color: #666; margin-bottom: 6px; }}
            .summary {{ font-size: 14px; margin-bottom: 6px; }}
            .plain-summary {{ font-size: 13px; color: #2563eb; margin: 8px 0; padding: 8px; background: #f0f7ff; border-radius: 4px; }}
            .links a {{ margin-right: 10px; }}
          </style>
        </head>
        <body>
          <h2>æ¯æ—¥AIè¶‹åŠ¿ç®€æŠ¥</h2>
          {''.join(items_html)}
        </body>
        </html>
        """

        (output_dir / "index.html").write_text(index_html, encoding="utf-8")

        safe_supabase_url = supabase_url.replace("http://", "https://", 1) if supabase_url.startswith("http://") else supabase_url

        # Favorite handler page (static)
        favorite_html = f"""
        <html>
        <head><meta charset='utf-8' /></head>
        <body>
          <h3>æ­£åœ¨æ”¶è—...</h3>
          <script>
            const params = new URLSearchParams(window.location.search);
            const id = params.get('id');
            if (!id) {{ document.body.innerHTML = '<h3>ç¼ºå°‘æ–‡ç« ID</h3>'; throw new Error('Missing article ID'); }}

            const supabaseUrl = '{safe_supabase_url}';
            const supabaseKey = '{supabase_anon_key}';
            
            // åˆå§‹åŒ– Supabase client
            const supabase = supabase.createClient(supabaseUrl, supabaseKey);

            // è°ƒç”¨ Edge Function
            async function processFavorite() {{
              try {{
                // å°è¯•åŒ¿åç™»å½•è·å– JWT
                const {{ data: {{ user }}, error: authError }} = await supabase.auth.signInAnonymously();
                
                if (authError || !user) {{
                  // å¦‚æœåŒ¿åç™»å½•å¤±è´¥ï¼Œç›´æ¥è°ƒç”¨ï¼ˆç”¨ service roleï¼‰
                  const response = await fetch(
                    supabaseUrl + '/functions/v1/process-favorite',
                    {{
                      method: 'POST',
                      headers: {{
                        'Authorization': 'Bearer ' + supabaseKey,
                        'Content-Type': 'application/json'
                      }},
                      body: JSON.stringify({{ article_id: id }})
                    }}
                  );
                  const result = await response.json();
                  if (result.success) {{
                    document.body.innerHTML = '<h3>æ”¶è—æˆåŠŸï¼Œç®€æå·²ç”Ÿæˆ</h3><p>' + (result.plain_summary || '') + '</p>';
                  }} else {{
                    document.body.innerHTML = '<h3>å¤„ç†å¤±è´¥: ' + (result.error || 'æœªçŸ¥é”™è¯¯') + '</h3>';
                  }}
                }} else {{
                  // åŒ¿åç™»å½•æˆåŠŸï¼Œç”¨ JWT è°ƒç”¨
                  const {{ data, error }} = await supabase.functions.invoke('process-favorite', {{
                    body: {{ article_id: id }}
                  }});
                  
                  if (error) {{
                    document.body.innerHTML = '<h3>é”™è¯¯: ' + error.message + '</h3>';
                  }} else if (data && data.success) {{
                    document.body.innerHTML = '<h3>æ”¶è—æˆåŠŸï¼Œç®€æå·²ç”Ÿæˆ</h3><p>' + (data.plain_summary || '') + '</p>';
                  }} else {{
                    document.body.innerHTML = '<h3>å¤„ç†å¤±è´¥</h3>';
                  }}
                }}
              }} catch (err) {{
                document.body.innerHTML = '<h3>é”™è¯¯: ' + err.message + '</h3>';
              }}
            }}

            processFavorite();
          </script>
        </body>
        </html>
        """

        (output_dir / "favorite.html").write_text(favorite_html, encoding="utf-8")
